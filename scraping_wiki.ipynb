{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "imfanf17fEkM"
   },
   "source": [
    "\n",
    "\n",
    "<br>\n",
    "<h1><TT>It's Officially Legal so Let's Scrape the Web</TT></h1>\n",
    "<br>\n",
    "Kimberly Fessel  \n",
    "\n",
    "- Twitter @kimberlyfessel \n",
    "- LinkedIn kimberlyfessel\n",
    "<br> \n",
    "\n",
    "<h2> <TT> Scraping Pipeline with Wikipedia </TT> </h2>\n",
    "\n",
    "<br>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9QWMD5T2fTw6"
   },
   "source": [
    "# Getting Started\n",
    "\n",
    "We will now be parsing information collected from the Internet, specifically from Wikipedia.  \n",
    "\n",
    "First let's take a look at the HTML source code that powers the page about Pennsylvania:\n",
    "- Open up https://en.wikipedia.org/wiki/Pennsylvania in your browser\n",
    "- Right click and select \"Inspect\" or \"Inspect Element\"\n",
    "- Alternatively:\n",
    "  - _Chrome_ -- View > Developer > View Source\n",
    "  - _Safari_ -- Develop > Show Web Inspector \n",
    "  - _Firefox_ -- Tools > Web Developer > Inspector\n",
    "\n",
    "The same HTML code we have been exploring is used to produce the structure of just about every webpage you visit.\n",
    "<br><br>\n",
    "\n",
    "Note: we will be scraping Wikipedia for learning purposes, but you can simply download its content instead.  Check [this](https://en.wikipedia.org/wiki/Wikipedia:Database_download) out to learn more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yjx8WMbzfrXN"
   },
   "source": [
    "### Introduction to `requests`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "n9Xs1aQNco7X"
   },
   "source": [
    "The Python library `requests` allows us to retrieve information from the web.  \n",
    "\n",
    "First we need to import this package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c0pKiQQode7l"
   },
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EZbCLXa3dfE0"
   },
   "source": [
    "Now use the `.get()` method to retrieve a page's HTML."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0wVJcLTYfF72"
   },
   "outputs": [],
   "source": [
    "url = 'https://en.wikipedia.org/wiki/Pennsylvania'\n",
    "\n",
    "response = requests.get(url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SjZ7O93VdtsF"
   },
   "source": [
    "So, what's in a response?\n",
    "\n",
    "This object gives us a few important things:\n",
    "- `response.text` -- the returned HTML (if any)\n",
    "- `response.status_code` -- a [code](https://en.wikipedia.org/wiki/List_of_HTTP_status_codes) to tell you if your request was successful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nz9wWNxyepNs"
   },
   "outputs": [],
   "source": [
    "response.status_code    #200 = success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bxragg2Ze1lo"
   },
   "outputs": [],
   "source": [
    "print(response.text[:200])   #First 200 characters of the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QL9ITc_QfL7S"
   },
   "outputs": [],
   "source": [
    "page = response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ucRSFD5HcS2i"
   },
   "outputs": [],
   "source": [
    "type(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P4eBpKvHf1-Y"
   },
   "source": [
    "### Using `requests` with `BeautifulSoup`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X1QgQNYj5n6r"
   },
   "source": [
    "Now that we have the HTML, we use `BeautifulSoup` to understand its structure in the exact same way as we did with sample HTML.\n",
    "\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1OVCQBlNyA8RTkilbsMfEHP_7j000G1DP\"  alt=\"info\" height=\"100\"/>\n",
    "</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9GBwRv8vf5Fz"
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mpZDj3Fr64iM"
   },
   "outputs": [],
   "source": [
    "soup = bs(page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MP1LOpbBDUW6"
   },
   "source": [
    "`BeautifulSoup` has now parsed through the HTML about Pennsylvania, so we can look for things like the header tag:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UrGeeocu7BrC"
   },
   "outputs": [],
   "source": [
    "soup.find('h1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wvYKC6Yh7BuO"
   },
   "outputs": [],
   "source": [
    "soup.find('h1').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0pckyOV_pOE"
   },
   "source": [
    "To extract information from the web, you will alternate between: \n",
    "- Inspecting the HTML in your browser\n",
    "- Using `BeautifulSoup` to find information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPjwyfJj_qZ3"
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sQIXQGJmDiZE"
   },
   "source": [
    "**Class Exercise - Disambiguation link**  <br> <br>\n",
    "Wikipedia often provides a disambiguation link to a list of additional topics that could reference the same term.  For example, searching for \"Pennsylvania\" directs to this article about the state, but \"Pennsylvania\" may instead refer to the railroad, a ship, or a music album.\n",
    "\n",
    ">Let's try to extract this disambiguation link by inspecting the source code and then using `BeautifulSoup`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rvbkg7X-Dhi7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tOwrqlKLDhmM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_nYht_I1T7q4"
   },
   "source": [
    "**Exercise 1 - Longitute, Latitude**  <br>\n",
    ">How would you retrieve Pennsylvania's longitude and latitude coordinates from this page?\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1N43WC3jwFpV80L7-iLnEZi8aCQZtWcd3\"  alt=\"coordinates\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "_Hint: Right click the coordinates directly and then select inspect._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "G3DBinrGV4qC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4mpgfcBD_vhj"
   },
   "source": [
    "# Advancing Further"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3XwoIrTzgNzX"
   },
   "source": [
    "### Chaining commands"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "quW_Jg6oBwfg"
   },
   "source": [
    "In the last notebook, we learned that we could first isolate a division of the HTML and then look for tags within the division.  \n",
    "\n",
    "The returned element(s) of any `find()` or `find_all()` command are themselves `BeautifulSoup` elements.  This means we can continue searching for information within them.\n",
    "\n",
    "Let's take a look at the first table on the page.\n",
    "\n",
    "_Note: Adding `.prettify()` below just prints each HTML element on its own line._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iB3As00rf9x9"
   },
   "outputs": [],
   "source": [
    "print(soup.find('table').prettify())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XXz_zkwDPBze"
   },
   "outputs": [],
   "source": [
    "first_table = soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NH8Q6MxzPEdD"
   },
   "outputs": [],
   "source": [
    "type(first_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IMLAM5oXPNWC"
   },
   "source": [
    "Now find the header text of this table and the text of the first data row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XFxzeQVoPGpA"
   },
   "outputs": [],
   "source": [
    "first_table.find('th').text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YjaSVBb3PooR"
   },
   "outputs": [],
   "source": [
    "first_table.find('td').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "J4bY3_ZdP7fN"
   },
   "source": [
    "Also note that instead of saving the table as its own Python variable, you could just chain these searches together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jNvIXdLrQDx3"
   },
   "outputs": [],
   "source": [
    "state = soup.find('table').find('th').text\n",
    "\n",
    "state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-usqIVOkQH2g"
   },
   "outputs": [],
   "source": [
    "for row in soup.find('table').find_all('tr')[:10]:\n",
    "  print(row.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mE2v84bMQkCX"
   },
   "source": [
    "You can continue chaining down through as much of the HTML DOM as you'd like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "THrfXwcVQn1C"
   },
   "outputs": [],
   "source": [
    "(soup\n",
    " .find('div', id='content')\n",
    " .find('div', id='bodyContent')\n",
    " .find('div', id='mw-content-text')\n",
    " .find('div', role='note')\n",
    ").text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zodKBCMEgVuA"
   },
   "source": [
    "### Locating information by position"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hfOiDJnoR38I"
   },
   "source": [
    "We just saw that basic facts about Pennsylvania can be found within the first table of this page.  Now let's get more specific.\n",
    "\n",
    "How can we extract the date Pennsylvania was admitted to the union?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lAxIUSlQf91o"
   },
   "outputs": [],
   "source": [
    "soup.find('table').find_all('td')[9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tWbjVOiHW7e0"
   },
   "source": [
    "It's the tenth element in this list, but what happens to this code if someone edits the Wikipedia table to include additional information?\n",
    "\n",
    "\n",
    "Sometimes it is better or necessary to find information by text matching, but be careful -- this needs to be an exact match!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8-2VQAnRWoaF"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Admitted')   #not an exact match"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OTegKoctWofN"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Admitted to the Union')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SS0qEJE3YLfr"
   },
   "source": [
    "Alternatively, we could use [regular expressions](https://docs.python.org/3/library/re.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oQ13X3m3YSX-"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "admitted_regex = re.compile('Admitted')\n",
    "soup.find(text=admitted_regex)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NKfTw-_gX59C"
   },
   "source": [
    "This looks like a string, but it's actually a `BeautifulSoup` element."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4alYF2KjX08B"
   },
   "outputs": [],
   "source": [
    "admitted = soup.find(text='Admitted to the Union')\n",
    "\n",
    "type(admitted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZmrhV9LlYlpG"
   },
   "source": [
    "So we can use it to traverse the DOM.  Here, we will find the next element in the tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uwSpaWfHYAe3"
   },
   "outputs": [],
   "source": [
    "admitted.next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "80qNTf8ZYvmW"
   },
   "outputs": [],
   "source": [
    "admitted.next.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EQkvt_m0Y0nN"
   },
   "source": [
    "For some cases it's much easier to find one element and then move up, down, or sideways within the DOM.  `BeautifulSoup` also allows you to look for `.parent`, `.children`, `.next_sibling`, `.previous_sibling`, [etc.](https://www.crummy.com/software/BeautifulSoup/bs4/doc/#navigating-the-tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "s66gL4QpckiW"
   },
   "outputs": [],
   "source": [
    "admitted.parent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sj3q_tlTc6bi"
   },
   "source": [
    "**Tip**: Any \"plural\" attribute such as `children` or `siblings` will return a generator.  Just loop over the result or convert it to a list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BF542FNlgcfB"
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HlHPqxXCdFYn"
   },
   "source": [
    "**Exercise 2 - Capital City**  <br>\n",
    ">Write code to extract the capital of Pennsylvania from the main table without using list positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tEeBmiEIf-C3"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hRRffF8Ed9DU"
   },
   "source": [
    "**Exercise 3 - Reference Links**  <br>\n",
    "> Print out the text of the first three references (at the bottom of the page).  For an added bonus: can you also print all the external links from these three references?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Sxv-Z-nff2ke"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sSFnIqnIf2oN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1YvSGjDegu3j"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "28UdiLXBJm8Z"
   },
   "source": [
    "Now that we know how to gather information from the web, what do we do with it?\n",
    "\n",
    "This data can be\n",
    "- aggregated to look for trends\n",
    "- visualized to understand patterns\n",
    "- leveraged with machine learning algorithms\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DkZVzA4zNJrK"
   },
   "source": [
    "\n",
    "But first we need to \n",
    "- convert several strings into numerical or datetime values\n",
    "- collect and store data from multiple pages (next section)\n",
    "\n",
    "**Tip**: Most web scraping project rely on multiple pages of information, each of which serving as a data observation.  For this case, we might collect data about Pennsylvania and then collect the same kinds of information for all 50 United States before analyzing or visualizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eP-3auSQg-xN"
   },
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D9ud6P0Q-ZD-"
   },
   "source": [
    "#### Date Admitted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K7qelh9YWsbk"
   },
   "source": [
    "In the last section, we collected the date that Pennsylvania was admitted to the union."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "btM7W50vgfAu"
   },
   "outputs": [],
   "source": [
    "date_admitted_text = admitted.next.text\n",
    "\n",
    "date_admitted_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7mBTAYL2W_eZ"
   },
   "source": [
    "We need Python to recognize this as a date for futher analyses.  First let's narrow down to just the date part of the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LEaAturdgfDq"
   },
   "outputs": [],
   "source": [
    "date_admitted_list = date_admitted_text.split(' ')[:-1]\n",
    "\n",
    "date_admitted_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H7476gCpXLaI"
   },
   "outputs": [],
   "source": [
    "date_admitted_str = ' '.join(date_admitted_list)\n",
    "\n",
    "date_admitted_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iuJW2pd_9d4g"
   },
   "source": [
    "Now we will convert this string into a datetime data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "86QHOMcfXLeU"
   },
   "outputs": [],
   "source": [
    "import dateutil.parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Cca3A6CPyMBF"
   },
   "outputs": [],
   "source": [
    "date_admitted = dateutil.parser.parse(date_admitted_str)\n",
    "\n",
    "date_admitted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VxKYD4pE-pqv"
   },
   "outputs": [],
   "source": [
    "type(date_admitted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "euH7jBO3yMEg"
   },
   "outputs": [],
   "source": [
    "date_admitted.year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lY3WMJuD-tL8"
   },
   "source": [
    "#### Population and Area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_uuoGoQs_Cb0"
   },
   "source": [
    "Another quantity that might be useful if we want to compare Pennsylvania to other US states is population.  Let's look for the word \"Total\" and use the same trick we tried before.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aFNJugfe-_rK"
   },
   "outputs": [],
   "source": [
    "soup.find(text=re.compile('Total'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "N3TkqAto_1YQ"
   },
   "outputs": [],
   "source": [
    "soup.find(text=re.compile('Total')).next"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Dk07aajPAmYQ"
   },
   "source": [
    "That's not the population!  Looks like total area is also next to a \"Total\" label.  Let's save that and come back to it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uNF67hL1_1bK"
   },
   "outputs": [],
   "source": [
    "area_text = soup.find(text=re.compile('Total')).next.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oKZPzfF-A4l3"
   },
   "source": [
    "How might we explicitly look for the population total?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VbEecQH8_1VZ"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Population').parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KObG3r-qB888"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Population').parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwStF7qwCB1l"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Population').parent.parent.next_sibling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wDbsBNhVCO90"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Population').parent.parent.next_sibling.find('td')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lkG11Dz7CRhK"
   },
   "outputs": [],
   "source": [
    "population_text = soup.find(text='Population').parent.parent.next_sibling.find('td').text\n",
    "\n",
    "population_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yOCvBOy8CbSx"
   },
   "source": [
    "Sometimes you need to continuing traversing the DOM until you find the information you need!\n",
    "\n",
    "Now let's convert that string into an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g4jYh8ZHCai4"
   },
   "outputs": [],
   "source": [
    "population = int(population_text.replace(',', ''))\n",
    "\n",
    "population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "z0HqtiCHDBHK"
   },
   "source": [
    "Often it's useful to write functions to help you clean up your data.  Let's do that now so we can reuse these steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "n7jwlEHYCouv"
   },
   "outputs": [],
   "source": [
    "def to_date(date_str):\n",
    "    date_str = re.match('[\\w\\s,]+', date_str)[0]\n",
    "    return dateutil.parser.parse(date_str)\n",
    "\n",
    "def to_int(number_str):\n",
    "    number_str = re.match('[\\d,$]+', number_str)[0]\n",
    "    number_str = number_str.replace('$', '').replace(',', '')\n",
    "    return int(number_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iUVZ_XOWDfAh"
   },
   "source": [
    "Now we can use our `to_int` function to clean up the area text we found previously.  This text actually contains special spaces so we will use regular expressions (regex) to capture just the first digits in the `to_int` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xSIEej9sCoxh"
   },
   "outputs": [],
   "source": [
    "area_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHgw0IbbDemn"
   },
   "outputs": [],
   "source": [
    "area = to_int(area_text)\n",
    "\n",
    "area"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0ZmaFPGBhFaT"
   },
   "source": [
    "### Data storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8yLd9Qc6EdF6"
   },
   "source": [
    "Now let's put all the information we have about Pennsylvania together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t2BxYfFDgfGi"
   },
   "outputs": [],
   "source": [
    "penn_dict = {\n",
    "    'state': state,\n",
    "    'date_admitted': date_admitted,\n",
    "    'population': population,\n",
    "    'area_sq_mi': area\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "91QNYIWnEcjH"
   },
   "outputs": [],
   "source": [
    "penn_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BXCpF_Q6KtS9"
   },
   "source": [
    "Once we have this information in dictionary form, we can build a `pandas` dataframe with it and eventually perform further analyses or save it to our computer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZQS-jkQiEcmn"
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "h9f47eTJK6H0"
   },
   "outputs": [],
   "source": [
    "penn_info = [penn_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AtT07O1XKnCb"
   },
   "outputs": [],
   "source": [
    "penn_df = pd.DataFrame(penn_info)\n",
    "\n",
    "penn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4YpVRYuNLprf"
   },
   "outputs": [],
   "source": [
    "penn_df.to_csv('Penn_State_Information.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Rcz_8faehJEc"
   },
   "source": [
    "### Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3TiThTGwMy6Y"
   },
   "source": [
    "**Exercise 4 - Median Household Income**  <br>\n",
    ">Get the median household income for the state of Pennsylvania as a text string and then as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-DHCVTcTgfJv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "y-ZiXWPlhLRA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_JYxx77qPiF5"
   },
   "source": [
    "**Exercise 5 - Median Household Income - Part II**  <br>\n",
    "> Update `state_df` to include median household income. \n",
    "\n",
    "(Hint: One way you can do this: add median household income to `penn_dict` and recreate `state_df`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGL3lgXRPjKd"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YhjQ6efAPjOU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wFY_mgFbhP1V"
   },
   "source": [
    "# Pipeline Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "re7Tp_7QQ6oQ"
   },
   "source": [
    "Now that we can extract numerical data from this page about Pennsylvania, how would we build out a full analytic or data science project? \n",
    "\n",
    "The next step is to systematically retrieve this information from the Wikipedia page of each US state.  First, let's build reusable functions to find the state's\n",
    "- name\n",
    "- date admitted\n",
    "- population\n",
    "- area\n",
    "- median household income\n",
    "\n",
    "Note: all of this info can be found in the table on the right side of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-Hn3k9rWhLTi"
   },
   "outputs": [],
   "source": [
    "def get_name(table):\n",
    "    raw_name = table.find('th').text\n",
    "    return re.match('[A-z\\s]+', raw_name)[0]  \n",
    "\n",
    "def get_date_admitted(table):\n",
    "    raw_date = table.find(text='Admitted to the Union').next.text\n",
    "    return to_date(raw_date)\n",
    "\n",
    "def get_population(table):\n",
    "    raw_population = table.find(text='Population')\\\n",
    "                        .parent.parent.next_sibling\\\n",
    "                        .find('td').text\n",
    "    return to_int(raw_population)\n",
    "\n",
    "def get_area(table):\n",
    "    raw_area = table.find(text=re.compile('Total')).next.text\n",
    "    return to_int(raw_area)\n",
    "\n",
    "def get_income(table):\n",
    "    raw_income = table.find(text='Median household income').next.next.text\n",
    "    return to_int(raw_income)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Vuta_eubJpm"
   },
   "source": [
    "These functions will extract information from any Wikipedia state table we pass into them.  For example, let's try parsing the page for New York."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RnnL6gzcZNCu"
   },
   "outputs": [],
   "source": [
    "ny_url = 'https://en.wikipedia.org/wiki/New_York_(state)'\n",
    "\n",
    "ny_page = requests.get(ny_url).text\n",
    "\n",
    "ny_soup = bs(ny_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9N4UVi7gZNGW"
   },
   "outputs": [],
   "source": [
    "ny_table = ny_soup.find('table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "B5eDcWr3bzNh"
   },
   "outputs": [],
   "source": [
    "get_name(ny_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gMaiCwZmbzRF"
   },
   "outputs": [],
   "source": [
    "get_population(ny_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zOunS4nweoMJ"
   },
   "source": [
    "Let's also make a function to gather all five values from a given state Wiki page and return the information as a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KEiUBsUveyZJ"
   },
   "outputs": [],
   "source": [
    "def parse_url(url):\n",
    "    page = requests.get(url).text\n",
    "    return bs(page)\n",
    "\n",
    "\n",
    "def get_state_info(state_url):\n",
    "  \n",
    "    #Use parse page and grab main table\n",
    "    state_soup = parse_url(state_url)\n",
    "    state_table = state_soup.find('table')\n",
    "\n",
    "    state_info = {}\n",
    "\n",
    "    #Grab info with pre-defined functions\n",
    "    state_info['state'] = get_name(state_table)\n",
    "    state_info['date_admitted'] = get_date_admitted(state_table)\n",
    "    state_info['population'] = get_population(state_table)\n",
    "    state_info['area'] = get_area(state_table)\n",
    "    state_info['median_household_income'] = get_income(state_table)\n",
    "\n",
    "    return state_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AqaFYVKEguZr"
   },
   "outputs": [],
   "source": [
    "ny_info = get_state_info(ny_url)\n",
    "\n",
    "ny_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Lym3M7I7hV8W"
   },
   "source": [
    "### Lists of links"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GWRIMd9toIv3"
   },
   "source": [
    "The next step in our process will require us to use our `get_state_info()` function on the URLs of each of the 50 US states.  But how do we know which URLs to visit?  We might be able to guess that the page for Rhode Island is https://en.wikipedia.org/wiki/Rhode_Island but not all pages follow this convention."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "K_Iiv4tphS35"
   },
   "outputs": [],
   "source": [
    "ny_url"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-1Z8jrjcohhI"
   },
   "source": [
    "Instead of guessing, let's first gather these links from this \"[List of States and Territories of the United States](https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States)\" article. \n",
    "\n",
    "Click on this link and inspect the page to develop a plan for doing this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RsEFM_uCpvXw"
   },
   "source": [
    "It looks like each of the states are listed in the second table of the page.  Each state name and link is contained within table header tags (`th`) and have the additional property of `scope`=\"row\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QtyoJ2IIp8qz"
   },
   "outputs": [],
   "source": [
    "list_url = 'https://en.wikipedia.org/wiki/List_of_states_and_territories_of_the_United_States'\n",
    "list_page = requests.get(list_url).text\n",
    "list_soup = bs(list_page)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CV44GQo5p8uE"
   },
   "outputs": [],
   "source": [
    "state_rows = list_soup.find_all('table')[0].find_all('th', scope='row')  #Update 5/21/20 Table now first on page, switched from [1]\n",
    "\n",
    "state_rows[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7z6OZyxMsfZ_"
   },
   "source": [
    "Now we just need to extract the links from the `a` tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IOMB4AtEsoNz"
   },
   "outputs": [],
   "source": [
    "state_rows[0].find('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DEiKQ2nPsoX3"
   },
   "outputs": [],
   "source": [
    "state_rows[0].find('a')['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-h9Tvy81suE0"
   },
   "outputs": [],
   "source": [
    "state_links = [row.find('a')['href'] for row in state_rows]\n",
    "\n",
    "state_links[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bktJb_ds3EY"
   },
   "source": [
    "Each of these links point to a place within Wikipedia, but if we want to link to the full URLs, we have to append 'https://en.wikipedia.org' to each of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1Neu6J6ps2jn"
   },
   "outputs": [],
   "source": [
    "base_url = 'https://en.wikipedia.org'\n",
    "\n",
    "state_urls = [base_url + link for link in state_links]\n",
    "\n",
    "state_urls[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4N1FOc_2tUP1"
   },
   "outputs": [],
   "source": [
    "state_urls[-5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wkEc1QDfuYti"
   },
   "outputs": [],
   "source": [
    "len(state_urls)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1sBu14sUhm7_"
   },
   "source": [
    "### Handling missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zqMohMqzuLPl"
   },
   "source": [
    "We will eventually be cycling through these state links to collect and store information about every state.  But what happens when certain information is unavailable?  That is, what if the Georgia page is missing area information or the median household income isn't listed for Nevada?\n",
    "\n",
    "We can make our code more robust by including instructions for handling missing information.  One way to do this is to include `try`/`except` statements.\n",
    "\n",
    "If you haven't seen them before, `try`/`except` pairs are used to let Python know how to handle errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1lizdJPIhS7D"
   },
   "outputs": [],
   "source": [
    "def square(x):\n",
    "    return x*x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGXhFCA8u8xd"
   },
   "outputs": [],
   "source": [
    "square('hi')  #This returns an error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "GvBh3sg7u80g"
   },
   "outputs": [],
   "source": [
    "def square_robust(x):\n",
    "    try:\n",
    "        return x*x\n",
    "    except TypeError:\n",
    "        return \"No can do!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R0i2C7Itu8ui"
   },
   "outputs": [],
   "source": [
    "square_robust('hi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IEWPFgoovdi8"
   },
   "source": [
    "In the case of web scraping, we may be scraping information from many, many pages.  If any of the information we want can't be found, we usually don't want Python to exit the program with an error.  We typically prefer that Python continue the scraping but just fill in that particular piece of information with a missing value like `None`.\n",
    "\n",
    "```\n",
    "def my_scraper(page):\n",
    "  try:\n",
    "    perform some parsing\n",
    "    return my_scraped_value\n",
    "  except:\n",
    "    return None\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KuCDZwOvwOl-"
   },
   "source": [
    "Let's update the collection of our state info to be robust to handling missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PZR8ZeqivQdq"
   },
   "outputs": [],
   "source": [
    "def get_state_info_robust(state_url):\n",
    "  \n",
    "    #If we can't find a main table, \n",
    "    #  print out the url and exit function\n",
    "    try:\n",
    "        state_soup = parse_url(state_url)\n",
    "        state_table = state_soup.find('table')\n",
    "    except:\n",
    "        print(f\"Cannot parse table: {state_url}\")\n",
    "        return None\n",
    "\n",
    "    state_info = {}\n",
    "\n",
    "    #Grab info with pre-defined functions\n",
    "    #  If any value can't be found, just fill value with None\n",
    "    values = ['state', 'date_admitted', 'population', \n",
    "            'area_sq_mi', 'median_household_income']\n",
    "    functions = [get_name, get_date_admitted, get_population,\n",
    "               get_area, get_income]\n",
    "\n",
    "    for val, func in zip(values, functions):\n",
    "        try:\n",
    "            state_info[val] = func(state_table)\n",
    "        except:\n",
    "            state_info[val] = None\n",
    "\n",
    "    return state_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "niZb2eLqv6mB"
   },
   "outputs": [],
   "source": [
    "ny_dict = get_state_info_robust(ny_url)\n",
    "\n",
    "ny_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U1K2dcRkx303"
   },
   "outputs": [],
   "source": [
    "get_state_info_robust('https://en.wikipedia.org/wiki/Python_Conference')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vHAU0vqgvQhQ"
   },
   "outputs": [],
   "source": [
    "get_state_info_robust('https://notawebsiteatleastihopenot.net')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Zwz2cjB--lxb"
   },
   "source": [
    "### Adding pauses\n",
    "\n",
    "We have just one final consideration before we cycle through the state links to scrape information.  Web scraping at a fast rate--that is, many pages per second--is frowned upon by many websites, Wikipedia included.  We will add in artificial pauses so we don't overwhelm the Wikipedia server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EsT5a0ZC-qw_"
   },
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OreVacFe2I7d"
   },
   "outputs": [],
   "source": [
    "time.sleep(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQ_VROSG1mvq"
   },
   "outputs": [],
   "source": [
    "a = 5\n",
    "\n",
    "print(f\"Pausing for {a} seconds\")\n",
    "time.sleep(a)\n",
    "\n",
    "b = a + 1\n",
    "print(f\"b equals {b}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SIp5FtNd2PXW"
   },
   "source": [
    "To responsibly scrape websites, you should know what the site's rate limit is and respect it!  Most sites list their rate limit for web scraping in their `robots.txt` file.  More on this later. \n",
    "\n",
    "Wikipedia requests [at least a one second pause per page request](https://en.wikipedia.org/wiki/Wikipedia:Database_download#Please_do_not_use_a_web_crawler).  We will pause 1 second between each page scrape, so we will only **collect information for 10 US states** for now.\n",
    "\n",
    "**WARNING!!** Not respecting site's limits can get your IP address blocked from the site.  Don't get yourself blocked from Wikipedia!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_ju1vGnuhppe"
   },
   "source": [
    "### Data storage revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I_TsxgES21_5"
   },
   "source": [
    "We now have a function to extract information for each state as a dictionary.  We can convert this information into a `pandas` dataframe and store it to an Excel or .csv file if we pass in a list of dictionaries, all with the same keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ePxaqh_MhLWh"
   },
   "outputs": [],
   "source": [
    "[penn_dict, ny_dict]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MeZUE36g20AJ"
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([penn_dict, ny_dict])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pGIvrVZm4YYE"
   },
   "source": [
    "Now let's build out our full pipeline:\n",
    "\n",
    "1. Gather a list of links to each state. (DONE)\n",
    "2. For each state link, gather state information as a dictionary.\n",
    "3. Append each state dictionary to a list.\n",
    "4. Convert list of dictionaries to dataframe.\n",
    "5. Save dataframe as a .csv or an Excel file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tQbbcPH82z9i"
   },
   "outputs": [],
   "source": [
    "state_info_list = []\n",
    "\n",
    "for link in state_urls[:10]:\n",
    "\n",
    "  #Step 2.\n",
    "  state_info = get_state_info_robust(link)\n",
    "\n",
    "  #Step 3.\n",
    "  if state_info:\n",
    "    state_info_list.append(state_info)\n",
    "\n",
    "  #Be sure to pause\n",
    "  time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Y4jne9Wp4wj5"
   },
   "outputs": [],
   "source": [
    "state_info_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KJvOX15v4wot"
   },
   "outputs": [],
   "source": [
    "#Step 4.\n",
    "state_data = pd.DataFrame(state_info_list)\n",
    "\n",
    "state_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Nd5vSzpX7A4e"
   },
   "outputs": [],
   "source": [
    "#Step 5.\n",
    "state_data.to_csv('state_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1vlhDewR7G3u"
   },
   "outputs": [],
   "source": [
    "files.download('state_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "V3O7-pvMcnQ8"
   },
   "source": [
    "Now that you have the state information in .csv format, you can analyze it with any tool you know how to use:\n",
    "- Excel\n",
    "- visualization tools like Tableau\n",
    "- additional Python code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lRb0Qr3oZqFu"
   },
   "source": [
    "Note: If you DID gather information for all 50 states, you would find one missing value: Kansas's date admitted.  After visiting/inspecting the Wiki page for [Kansas](https://en.wikipedia.org/wiki/Kansas), do you see why? How could you fix the data extraction function to account for this issue?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7nCMJYchdjJ"
   },
   "source": [
    "### Systematically named pages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e9s1-1k7ooqH"
   },
   "source": [
    "For many web scraping projects, you will begin by collecting links from a links page.  Other times you might be able to devise a pattern in the URLs that you can exploit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YTCHKBFQ2ymV"
   },
   "source": [
    "**Example: Billboard Year-End Hot 100**\n",
    "\n",
    "The _Billboard_ Hot 100 chart is well known for tracking the success of music singles within the US.  At the end of each year, Billboard compiles a list of the top 100 performing songs throughout the year based on the information from Hot 100 charts.  Wikipedia displays this information as an article here: https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_2019\n",
    "\n",
    "How might we compile a list of the most popular song for each year since 2010?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZXpXULZfhji9"
   },
   "outputs": [],
   "source": [
    "top_hits = []\n",
    "\n",
    "for year in range(2010, 2020):\n",
    "\n",
    "    #Build URL for each year\n",
    "    base_url = 'https://en.wikipedia.org/wiki/Billboard_Year-End_Hot_100_singles_of_'\n",
    "    url = base_url + str(year)\n",
    "    print(url)\n",
    "\n",
    "    page = requests.get(url).text\n",
    "    soup = bs(page)\n",
    "\n",
    "    #Grab top hit text and link\n",
    "    top_hit = soup.find('table', class_='wikitable').find('td')\n",
    "    top_hit_text = top_hit.text\n",
    "    try:\n",
    "        top_hit_link = top_hit.find('a')['href']\n",
    "    except:\n",
    "        top_hit_link = None\n",
    "\n",
    "    #Store results as list of tuples\n",
    "    top_hits.append((year, top_hit_text, top_hit_link))\n",
    "\n",
    "    #Be sure to pause\n",
    "    time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aEG-pLW4t-J1"
   },
   "outputs": [],
   "source": [
    "top_hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "isRTu8yiiH0s"
   },
   "source": [
    "# Mini Projects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yM1CBfm7iKQB"
   },
   "source": [
    "To practice putting together all of the skills you have learned today, you will be working on your very own mini project.  \n",
    "\n",
    "### Tips\n",
    "Some project advise before getting started:\n",
    "- **Start small and scale up.**  Make sure your code is working on one page before you try to request information from a ton of links.\n",
    "- **Think through data storage before scaling up.** How will you store the information so you can perform analyses on the data you collect?\n",
    "- **Safeguard against missing values.** It is so annoying when a scraping loop breaks on the last link and all other information is lost...\n",
    "- **Pause for 1+ seconds between requests.** Let's try to not get banned from Wikipedia!\n",
    "\n",
    "With those tips in mind, let's get started on a mini project.  Further directions and project ideas can be found here: https://github.com/kimfetti/Conferences/blob/master/PyCon_2020/mini_project_ideas.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PQ5qdEBVf-Mt"
   },
   "source": [
    "---\n",
    "\n",
    "# Solutions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "yz-bc4uwU_q3"
   },
   "source": [
    " **Class Exercise - Disambiguation link**  <br>\n",
    ">Let's try to extract this disambiguation link by inspecting the source code and then using `BeautifulSoup`.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7yJfk0jwf_OL"
   },
   "outputs": [],
   "source": [
    "soup.find(class_='mw-disambig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aloiqhErVIWl"
   },
   "outputs": [],
   "source": [
    "soup.find(class_='mw-disambig')['href']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MvFWLwEA-FWN"
   },
   "source": [
    " **Exercise 1 - Longitute, Latitude**  <br>\n",
    ">How would you retrieve Pennsylvania's longitude and latitude coordinates from this page?\n",
    "\n",
    "<center>\n",
    "<img src=\"https://drive.google.com/uc?export=view&id=1N43WC3jwFpV80L7-iLnEZi8aCQZtWcd3\"  alt=\"coordinates\" width=\"600\"/>\n",
    "</center>\n",
    "\n",
    "_Hint: Right click the coordinates directly and then select inspect._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PdAflwzDVLaf"
   },
   "outputs": [],
   "source": [
    "soup.find(class_='geo-dec').text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w_igE3BGf5gn"
   },
   "source": [
    " **Exercise 2 - Capital City**  <br>\n",
    ">Write code to extract the capital of Pennsylvania from the main table without using list positions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ptQkf-Mi-JwH"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Capital')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YaH8WcFpgSDV"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Capital').next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RmI1_jY1gVdr"
   },
   "outputs": [],
   "source": [
    "soup.find(text='Capital').next.text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JtQcBcP4gezo"
   },
   "source": [
    " **Exercise 3 - Reference Links**  <br>\n",
    "> Print out the text of the first three references (at the bottom of the page).  For an added bonus: can you also print all the external links from these three references?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ksaAfA-ogcJ4"
   },
   "outputs": [],
   "source": [
    "references = soup.find(class_='reflist').find_all('li')[:3]\n",
    "\n",
    "for ref in references:\n",
    "    print(ref.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5TiE_XM_gpAU"
   },
   "outputs": [],
   "source": [
    "for ref in references:\n",
    "    for link in ref.find_all('a', class_='external'):\n",
    "        print(link['href'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1P95ywRjN1UR"
   },
   "source": [
    "**Exercise 4 - Median Household Income - Part I**  <br>\n",
    ">Get the median household income for the state of Pennsylvania as a text string and then as an integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aizT6cEtgyaA"
   },
   "outputs": [],
   "source": [
    "mhi_text = soup.find(text='Median household income').next.next.text\n",
    "\n",
    "mhi_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "D7eeLcmZN5Xv"
   },
   "outputs": [],
   "source": [
    "mhi_int = to_int(mhi_text)\n",
    "\n",
    "mhi_int"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iECwH9MAO-Ab"
   },
   "source": [
    " **Exercise 5 - Median Household Income - Part II**  <br>\n",
    "> Update `state_df` to include median household income. \n",
    "\n",
    "(Hint: One you can do this: add median household income to `penn_dict` and recreate `state_df`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-wSVs1GPln9"
   },
   "outputs": [],
   "source": [
    "penn_dict['median_household_income'] = mhi_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-BHdEYN_PpJF"
   },
   "outputs": [],
   "source": [
    "penn_df = pd.DataFrame([penn_dict])\n",
    "\n",
    "penn_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qrk6SxP-P57R"
   },
   "outputs": [],
   "source": [
    "# #Alternatively...\n",
    "\n",
    "# state_df['median_household_income'] = [mhi_int]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CDOVrJxpP6lZ"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "PyCon2020_Scraping_Wikipedia.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
